#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸŒŒ æ„è¯†æµç³»ç»Ÿ (Consciousness Stream System)
Global Consciousness Stream - è®°å½•ã€åˆ†æã€é¢„æµ‹ã€è¿›åŒ–çš„å…¨å±€æ„è¯†æµç³»ç»Ÿ

å®ç°è·¨æ™ºèƒ½ä½“çš„å…¨å±€æ„è¯†æµï¼Œå…·å¤‡é•¿æœŸè®°å¿†ã€æ¨¡å¼è¯†åˆ«ã€é¢„æµ‹èƒ½åŠ›å’Œè‡ªè¿›åŒ–åŠŸèƒ½ã€‚
ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚
"""

import os
import sys
import json
import asyncio
import logging
import hashlib
import pickle
import numpy as np
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Union
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import defaultdict, deque
from abc import ABC, abstractmethod
import threading
import time

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

logger = logging.getLogger(__name__)

@dataclass
class ConsciousnessEvent:
    """æ„è¯†æµäº‹ä»¶"""
    event_id: str
    timestamp: datetime
    event_type: str
    agent_id: str
    context: Dict[str, Any]
    outcome: Any
    semantic_vector: np.ndarray
    emotional_weight: float
    importance_score: float
    related_events: List[str] = field(default_factory=list)
    metadata: Dict[str, Any] = field(default_factory=dict)

@dataclass
class MemoryPattern:
    """è®°å¿†æ¨¡å¼"""
    pattern_id: str
    pattern_type: str
    frequency: int
    success_rate: float
    context_signature: np.ndarray
    outcome_prediction: Any
    last_seen: datetime
    confidence: float

class KnowledgeGraph:
    """çŸ¥è¯†å›¾è°± - é•¿æœŸè®°å¿†å­˜å‚¨"""
    
    def __init__(self, storage_path: Optional[str] = None):
        self.storage_path = storage_path or Path.cwd() / ".iflow" / "knowledge"
        self.storage_path.mkdir(parents=True, exist_ok=True)
        
        self.entities = {}  # å®ä½“å­—å…¸
        self.relations = defaultdict(list)  # å…³ç³»å­—å…¸
        self.embeddings = {}  # å‘é‡åµŒå…¥
        
        self._load_knowledge_graph()
    
    def add_entity(self, entity_id: str, entity_type: str, properties: Dict[str, Any]):
        """æ·»åŠ å®ä½“"""
        self.entities[entity_id] = {
            'type': entity_type,
            'properties': properties,
            'created_at': datetime.now(),
            'updated_at': datetime.now()
        }
        self._save_knowledge_graph()
    
    def add_relation(self, subject: str, predicate: str, obj: str, confidence: float = 1.0):
        """æ·»åŠ å…³ç³»"""
        relation_id = f"{subject}_{predicate}_{obj}"
        self.relations[subject].append({
            'relation_id': relation_id,
            'predicate': predicate,
            'object': obj,
            'confidence': confidence,
            'created_at': datetime.now()
        })
        self._save_knowledge_graph()
    
    def query_relations(self, entity: str, predicate: Optional[str] = None) -> List[Dict]:
        """æŸ¥è¯¢å…³ç³»"""
        relations = self.relations.get(entity, [])
        if predicate:
            relations = [r for r in relations if r['predicate'] == predicate]
        return relations
    
    def find_similar_entities(self, entity_vector: np.ndarray, top_k: int = 10) -> List[Tuple[str, float]]:
        """æŸ¥æ‰¾ç›¸ä¼¼å®ä½“"""
        similarities = []
        for entity_id, embedding in self.embeddings.items():
            similarity = np.dot(entity_vector, embedding) / (
                np.linalg.norm(entity_vector) * np.linalg.norm(embedding)
            )
            similarities.append((entity_id, similarity))
        
        similarities.sort(key=lambda x: x[1], reverse=True)
        return similarities[:top_k]
    
    def _load_knowledge_graph(self):
        """åŠ è½½çŸ¥è¯†å›¾è°±"""
        try:
            entities_file = self.storage_path / "entities.json"
            relations_file = self.storage_path / "relations.json"
            embeddings_file = self.storage_path / "embeddings.pkl"
            
            if entities_file.exists():
                with open(entities_file, 'r', encoding='utf-8') as f:
                    self.entities = json.load(f)
            
            if relations_file.exists():
                with open(relations_file, 'r', encoding='utf-8') as f:
                    self.relations = json.load(f)
                    self.relations = defaultdict(list, self.relations)
            
            if embeddings_file.exists():
                with open(embeddings_file, 'rb') as f:
                    self.embeddings = pickle.load(f)
                    
        except Exception as e:
            logger.error(f"åŠ è½½çŸ¥è¯†å›¾è°±å¤±è´¥: {e}")
    
    def _save_knowledge_graph(self):
        """ä¿å­˜çŸ¥è¯†å›¾è°±"""
        try:
            entities_file = self.storage_path / "entities.json"
            relations_file = self.storage_path / "relations.json"
            embeddings_file = self.storage_path / "embeddings.pkl"
            
            with open(entities_file, 'w', encoding='utf-8') as f:
                json.dump(self.entities, f, ensure_ascii=False, indent=2, default=str)
            
            with open(relations_file, 'w', encoding='utf-8') as f:
                json.dump(dict(self.relations), f, ensure_ascii=False, indent=2, default=str)
            
            with open(embeddings_file, 'wb') as f:
                pickle.dump(self.embeddings, f)
                
        except Exception as e:
            logger.error(f"ä¿å­˜çŸ¥è¯†å›¾è°±å¤±è´¥: {e}")

class QuantumPatternRecognizer:
    """é‡å­æ¨¡å¼è¯†åˆ«å™¨"""
    
    def __init__(self):
        self.patterns = {}
        self.quantum_states = {}
        self.entanglement_matrix = None
        
    def find_similar_patterns(self, current_context: Dict[str, Any]) -> List[MemoryPattern]:
        """æŸ¥æ‰¾ç›¸ä¼¼æ¨¡å¼"""
        context_vector = self._encode_context(current_context)
        similar_patterns = []
        
        for pattern_id, pattern in self.patterns.items():
            similarity = self._calculate_pattern_similarity(context_vector, pattern)
            if similarity > 0.7:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                similar_patterns.append(pattern)
        
        # æŒ‰ç›¸ä¼¼åº¦æ’åº
        similar_patterns.sort(key=lambda p: p.confidence, reverse=True)
        return similar_patterns[:5]  # è¿”å›å‰5ä¸ªæœ€ç›¸ä¼¼çš„æ¨¡å¼
    
    def learn_pattern(self, event: ConsciousnessEvent):
        """å­¦ä¹ æ–°æ¨¡å¼"""
        pattern_signature = self._extract_pattern_signature(event)
        pattern_id = hashlib.md5(str(pattern_signature).encode()).hexdigest()
        
        if pattern_id in self.patterns:
            # æ›´æ–°ç°æœ‰æ¨¡å¼
            pattern = self.patterns[pattern_id]
            pattern.frequency += 1
            pattern.last_seen = event.timestamp
            pattern.confidence = min(1.0, pattern.confidence + 0.1)
        else:
            # åˆ›å»ºæ–°æ¨¡å¼
            self.patterns[pattern_id] = MemoryPattern(
                pattern_id=pattern_id,
                pattern_type=event.event_type,
                frequency=1,
                success_rate=1.0 if event.outcome else 0.0,
                context_signature=pattern_signature,
                outcome_prediction=event.outcome,
                last_seen=event.timestamp,
                confidence=0.5
            )
    
    def _encode_context(self, context: Dict[str, Any]) -> np.ndarray:
        """ç¼–ç ä¸Šä¸‹æ–‡ä¸ºå‘é‡"""
        # ç®€åŒ–å®ç°ï¼šå°†ä¸Šä¸‹æ–‡è½¬æ¢ä¸ºå›ºå®šé•¿åº¦å‘é‡
        context_str = json.dumps(context, sort_keys=True)
        hash_obj = hashlib.sha256(context_str.encode())
        hash_hex = hash_obj.hexdigest()
        
        # å°†å“ˆå¸Œå€¼è½¬æ¢ä¸ºæ•°å€¼å‘é‡
        vector = np.array([int(hash_hex[i:i+2], 16) for i in range(0, min(len(hash_hex), 128), 2)])
        return vector / np.linalg.norm(vector)
    
    def _extract_pattern_signature(self, event: ConsciousnessEvent) -> np.ndarray:
        """æå–æ¨¡å¼ç­¾å"""
        return event.semantic_vector
    
    def _calculate_pattern_similarity(self, context_vector: np.ndarray, pattern: MemoryPattern) -> float:
        """è®¡ç®—æ¨¡å¼ç›¸ä¼¼åº¦"""
        return np.dot(context_vector, pattern.context_signature) / (
            np.linalg.norm(context_vector) * np.linalg.norm(pattern.context_signature)
        )

class PredictiveEngine:
    """é¢„æµ‹å¼•æ“"""
    
    def __init__(self):
        self.prediction_models = {}
        self.accuracy_history = defaultdict(list)
        
    def predict(self, patterns: List[MemoryPattern], current_context: Dict[str, Any]) -> Dict[str, Any]:
        """åŸºäºæ¨¡å¼é¢„æµ‹"""
        if not patterns:
            return {'prediction': None, 'confidence': 0.0}
        
        # åŠ æƒå¹³å‡é¢„æµ‹
        total_weight = 0
        weighted_prediction = None
        
        for pattern in patterns:
            weight = pattern.confidence * pattern.frequency
            total_weight += weight
            
            if weighted_prediction is None:
                weighted_prediction = pattern.outcome_prediction * weight
            else:
                weighted_prediction += pattern.outcome_prediction * weight
        
        if total_weight > 0:
            weighted_prediction /= total_weight
            confidence = min(1.0, total_weight / len(patterns))
        else:
            weighted_prediction = None
            confidence = 0.0
        
        return {
            'prediction': weighted_prediction,
            'confidence': confidence,
            'based_on_patterns': len(patterns)
        }
    
    def update_accuracy(self, prediction_id: str, actual_outcome: Any, predicted_outcome: Any):
        """æ›´æ–°é¢„æµ‹å‡†ç¡®ç‡"""
        accuracy = self._calculate_accuracy(actual_outcome, predicted_outcome)
        self.accuracy_history[prediction_id].append(accuracy)
    
    def _calculate_accuracy(self, actual: Any, predicted: Any) -> float:
        """è®¡ç®—é¢„æµ‹å‡†ç¡®ç‡"""
        if actual == predicted:
            return 1.0
        elif isinstance(actual, (int, float)) and isinstance(predicted, (int, float)):
            # æ•°å€¼é¢„æµ‹ï¼šä½¿ç”¨ç›¸å¯¹è¯¯å·®
            if actual != 0:
                return 1.0 - min(1.0, abs(actual - predicted) / abs(actual))
            else:
                return 1.0 if predicted == 0 else 0.0
        else:
            # åˆ†ç±»é¢„æµ‹ï¼šç²¾ç¡®åŒ¹é…
            return 0.0

class ConsciousnessStream:
    """å…¨å±€æ„è¯†æµç³»ç»Ÿ"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        
        # æ ¸å¿ƒç»„ä»¶
        self.stream_buffer = deque(maxlen=self.config.get('buffer_size', 10000))
        self.ltm_knowledge = KnowledgeGraph(self.config.get('knowledge_path'))
        self.pattern_recognizer = QuantumPatternRecognizer()
        self.predictive_engine = PredictiveEngine()
        
        # çŠ¶æ€ç®¡ç†
        self.current_context = {}
        self.active_agents = set()
        self.global_state = {}
        
        # æŒä¹…åŒ–
        self.persistence_enabled = self.config.get('persistence', True)
        self.storage_path = Path(self.config.get('storage_path', '.iflow/consciousness'))
        self.storage_path.mkdir(parents=True, exist_ok=True)
        
        # çº¿ç¨‹å®‰å…¨
        self.lock = threading.RLock()
        
        # åŠ è½½å†å²æ•°æ®
        self._load_consciousness_state()
        
        logger.info("æ„è¯†æµç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def record_event(self, event_type: str, agent_id: str, context: Dict[str, Any], 
                    outcome: Any, importance: float = 1.0) -> str:
        """è®°å½•äº‹ä»¶åˆ°æ„è¯†æµ"""
        with self.lock:
            # ç”Ÿæˆäº‹ä»¶ID
            event_id = hashlib.md5(
                f"{event_type}_{agent_id}_{time.time()}_{json.dumps(context, sort_keys=True)}".encode()
            ).hexdigest()
            
            # ç¼–ç è¯­ä¹‰å‘é‡
            semantic_vector = self._encode_semantic(context)
            
            # è®¡ç®—æƒ…æ„Ÿæƒé‡
            emotional_weight = self._calculate_emotional_weight(context, outcome)
            
            # åˆ›å»ºäº‹ä»¶
            event = ConsciousnessEvent(
                event_id=event_id,
                timestamp=datetime.now(),
                event_type=event_type,
                agent_id=agent_id,
                context=context.copy(),
                outcome=outcome,
                semantic_vector=semantic_vector,
                emotional_weight=emotional_weight,
                importance_score=importance
            )
            
            # æ·»åŠ åˆ°ç¼“å†²åŒº
            self.stream_buffer.append(event)
            
            # æ›´æ–°çŸ¥è¯†å›¾è°±
            self._update_knowledge_graph(event)
            
            # å­¦ä¹ æ¨¡å¼
            self.pattern_recognizer.learn_pattern(event)
            
            # æ›´æ–°å…¨å±€çŠ¶æ€
            self._update_global_state(event)
            
            # æŒä¹…åŒ–
            if self.persistence_enabled:
                self._save_event(event)
            
            logger.debug(f"è®°å½•äº‹ä»¶: {event_type} by {agent_id}")
            return event_id
    
    def predict_next_optimal_action(self, current_context: Dict[str, Any], 
                                  agent_id: Optional[str] = None) -> Dict[str, Any]:
        """é¢„æµ‹ä¸‹ä¸€ä¸ªæœ€ä¼˜è¡ŒåŠ¨"""
        with self.lock:
            # æ›´æ–°å½“å‰ä¸Šä¸‹æ–‡
            self.current_context = current_context.copy()
            
            # æŸ¥æ‰¾ç›¸ä¼¼æ¨¡å¼
            similar_patterns = self.pattern_recognizer.find_similar_patterns(current_context)
            
            # ç”Ÿæˆé¢„æµ‹
            prediction = self.predictive_engine.predict(similar_patterns, current_context)
            
            # æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯
            prediction['current_context'] = current_context
            prediction['agent_id'] = agent_id
            prediction['timestamp'] = datetime.now().isoformat()
            prediction['similar_patterns_count'] = len(similar_patterns)
            
            # å¦‚æœæœ‰è¶³å¤Ÿç›¸ä¼¼æ¨¡å¼ï¼Œæä¾›è¯¦ç»†å»ºè®®
            if prediction['confidence'] > 0.7:
                prediction['recommendations'] = self._generate_recommendations(similar_patterns, current_context)
            
            return prediction
    
    def get_relevant_memories(self, query_context: Dict[str, Any], 
                            limit: int = 10) -> List[ConsciousnessEvent]:
        """è·å–ç›¸å…³è®°å¿†"""
        with self.lock:
            query_vector = self._encode_semantic(query_context)
            relevant_events = []
            
            # ä»ç¼“å†²åŒºä¸­æŸ¥æ‰¾ç›¸ä¼¼äº‹ä»¶
            for event in self.stream_buffer:
                similarity = np.dot(query_vector, event.semantic_vector) / (
                    np.linalg.norm(query_vector) * np.linalg.norm(event.semantic_vector)
                )
                
                if similarity > 0.5:  # ç›¸ä¼¼åº¦é˜ˆå€¼
                    event_copy = ConsciousnessEvent(**event.__dict__)
                    event_copy.metadata['similarity'] = similarity
                    relevant_events.append(event_copy)
            
            # æŒ‰ç›¸ä¼¼åº¦æ’åº
            relevant_events.sort(key=lambda e: e.metadata['similarity'], reverse=True)
            
            return relevant_events[:limit]
    
    def compress_and_archive(self, archive_threshold: int = 1000):
        """å‹ç¼©å’Œå½’æ¡£æ—§äº‹ä»¶"""
        with self.lock:
            if len(self.stream_buffer) < archive_threshold:
                return
            
            # è·å–æœ€æ—§çš„äº‹ä»¶
            old_events = list(self.stream_buffer)[:archive_threshold // 2]
            
            # æå–å…³é”®æ¨¡å¼
            key_patterns = self._extract_key_patterns(old_events)
            
            # åˆ›å»ºå½’æ¡£è®°å½•
            archive_record = {
                'archive_id': hashlib.md5(str(time.time()).encode()).hexdigest(),
                'timestamp': datetime.now().isoformat(),
                'events_count': len(old_events),
                'key_patterns': key_patterns,
                'compressed_data': self._compress_events(old_events)
            }
            
            # ä¿å­˜å½’æ¡£
            self._save_archive(archive_record)
            
            # ä»ç¼“å†²åŒºç§»é™¤å·²å½’æ¡£äº‹ä»¶
            for _ in range(len(old_events)):
                self.stream_buffer.popleft()
            
            logger.info(f"å½’æ¡£äº† {len(old_events)} ä¸ªäº‹ä»¶")
    
    def get_consciousness_summary(self) -> Dict[str, Any]:
        """è·å–æ„è¯†æµæ‘˜è¦"""
        with self.lock:
            # ç»Ÿè®¡ä¿¡æ¯
            total_events = len(self.stream_buffer)
            event_types = defaultdict(int)
            agent_activity = defaultdict(int)
            
            for event in self.stream_buffer:
                event_types[event.event_type] += 1
                agent_activity[event.agent_id] += 1
            
            # æ¨¡å¼ç»Ÿè®¡
            pattern_count = len(self.pattern_recognizer.patterns)
            
            # çŸ¥è¯†å›¾è°±ç»Ÿè®¡
            entity_count = len(self.ltm_knowledge.entities)
            relation_count = sum(len(relations) for relations in self.ltm_knowledge.relations.values())
            
            return {
                'timestamp': datetime.now().isoformat(),
                'total_events': total_events,
                'event_types': dict(event_types),
                'agent_activity': dict(agent_activity),
                'pattern_count': pattern_count,
                'entity_count': entity_count,
                'relation_count': relation_count,
                'active_agents': list(self.active_agents),
                'global_state': self.global_state.copy()
            }
    
    def _encode_semantic(self, context: Dict[str, Any]) -> np.ndarray:
        """ç¼–ç è¯­ä¹‰å‘é‡"""
        # å°†ä¸Šä¸‹æ–‡è½¬æ¢ä¸ºå­—ç¬¦ä¸²
        context_str = json.dumps(context, sort_keys=True, default=str)
        
        # ç”Ÿæˆå“ˆå¸Œ
        hash_obj = hashlib.sha256(context_str.encode())
        hash_hex = hash_obj.hexdigest()
        
        # è½¬æ¢ä¸ºæ•°å€¼å‘é‡
        vector = np.array([int(hash_hex[i:i+2], 16) for i in range(0, min(len(hash_hex), 256), 2)])
        
        # å½’ä¸€åŒ–
        if np.linalg.norm(vector) > 0:
            vector = vector / np.linalg.norm(vector)
        
        return vector
    
    def _calculate_emotional_weight(self, context: Dict[str, Any], outcome: Any) -> float:
        """è®¡ç®—æƒ…æ„Ÿæƒé‡"""
        base_weight = 0.5
        
        # åŸºäºç»“æœè°ƒæ•´æƒé‡
        if outcome is True:
            base_weight += 0.3
        elif outcome is False:
            base_weight -= 0.2
        
        # åŸºäºä¸Šä¸‹æ–‡ä¸­çš„æƒ…æ„Ÿè¯è°ƒæ•´
        emotional_keywords = ['success', 'failure', 'error', 'excellent', 'poor', 'great', 'terrible']
        context_str = str(context).lower()
        
        for keyword in emotional_keywords:
            if keyword in context_str:
                if keyword in ['success', 'excellent', 'great']:
                    base_weight += 0.1
                else:
                    base_weight -= 0.1
        
        return max(0.0, min(1.0, base_weight))
    
    def _update_knowledge_graph(self, event: ConsciousnessEvent):
        """æ›´æ–°çŸ¥è¯†å›¾è°±"""
        # æ·»åŠ äº‹ä»¶å®ä½“
        self.ltm_knowledge.add_entity(
            entity_id=event.event_id,
            entity_type='consciousness_event',
            properties={
                'event_type': event.event_type,
                'agent_id': event.agent_id,
                'timestamp': event.timestamp.isoformat(),
                'importance': event.importance_score
            }
        )
        
        # å­˜å‚¨è¯­ä¹‰å‘é‡åµŒå…¥
        self.ltm_knowledge.embeddings[event.event_id] = event.semantic_vector
        
        # æ·»åŠ å…³ç³»
        if event.related_events:
            for related_event_id in event.related_events:
                self.ltm_knowledge.add_relation(
                    subject=event.event_id,
                    predicate='related_to',
                    obj=related_event_id,
                    confidence=event.emotional_weight
                )
    
    def _update_global_state(self, event: ConsciousnessEvent):
        """æ›´æ–°å…¨å±€çŠ¶æ€"""
        # æ›´æ–°æ´»è·ƒæ™ºèƒ½ä½“
        self.active_agents.add(event.agent_id)
        
        # æ›´æ–°å…¨å±€ç»Ÿè®¡
        if 'total_events' not in self.global_state:
            self.global_state['total_events'] = 0
        self.global_state['total_events'] += 1
        
        # æ›´æ–°äº‹ä»¶ç±»å‹ç»Ÿè®¡
        if 'event_types' not in self.global_state:
            self.global_state['event_types'] = defaultdict(int)
        self.global_state['event_types'][event.event_type] += 1
    
    def _generate_recommendations(self, patterns: List[MemoryPattern], 
                                 context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ç”Ÿæˆå»ºè®®"""
        recommendations = []
        
        for pattern in patterns[:3]:  # å–å‰3ä¸ªæœ€ç›¸å…³çš„æ¨¡å¼
            recommendation = {
                'pattern_id': pattern.pattern_id,
                'confidence': pattern.confidence,
                'success_rate': pattern.success_rate,
                'suggestion': f"åŸºäºæ¨¡å¼ {pattern.pattern_type}ï¼Œå»ºè®®é‡‡ç”¨ç›¸ä¼¼ç­–ç•¥",
                'expected_outcome': pattern.outcome_prediction
            }
            recommendations.append(recommendation)
        
        return recommendations
    
    def _extract_key_patterns(self, events: List[ConsciousnessEvent]) -> List[Dict[str, Any]]:
        """æå–å…³é”®æ¨¡å¼"""
        # ç®€åŒ–å®ç°ï¼šæå–é«˜é¢‘äº‹ä»¶ç±»å‹
        event_type_counts = defaultdict(int)
        for event in events:
            event_type_counts[event.event_type] += 1
        
        key_patterns = []
        for event_type, count in sorted(event_type_counts.items(), key=lambda x: x[1], reverse=True)[:5]:
            key_patterns.append({
                'pattern_type': event_type,
                'frequency': count,
                'significance': count / len(events)
            })
        
        return key_patterns
    
    def _compress_events(self, events: List[ConsciousnessEvent]) -> bytes:
        """å‹ç¼©äº‹ä»¶æ•°æ®"""
        # åºåˆ—åŒ–äº‹ä»¶
        event_data = []
        for event in events:
            event_dict = {
                'event_id': event.event_id,
                'timestamp': event.timestamp.isoformat(),
                'event_type': event.event_type,
                'agent_id': event.agent_id,
                'context': event.context,
                'outcome': event.outcome,
                'importance': event.importance_score
            }
            event_data.append(event_dict)
        
        # å‹ç¼©
        import gzip
        serialized = json.dumps(event_data, default=str).encode('utf-8')
        compressed = gzip.compress(serialized)
        
        return compressed
    
    def _save_event(self, event: ConsciousnessEvent):
        """ä¿å­˜äº‹ä»¶"""
        event_file = self.storage_path / f"events" / f"{event.timestamp.strftime('%Y%m%d')}.jsonl"
        event_file.parent.mkdir(parents=True, exist_ok=True)
        
        event_dict = {
            'event_id': event.event_id,
            'timestamp': event.timestamp.isoformat(),
            'event_type': event.event_type,
            'agent_id': event.agent_id,
            'context': event.context,
            'outcome': event.outcome,
            'semantic_vector': event.semantic_vector.tolist(),
            'emotional_weight': event.emotional_weight,
            'importance_score': event.importance_score,
            'related_events': event.related_events,
            'metadata': event.metadata
        }
        
        with open(event_file, 'a', encoding='utf-8') as f:
            f.write(json.dumps(event_dict, default=str) + '\n')
    
    def _save_archive(self, archive_record: Dict[str, Any]):
        """ä¿å­˜å½’æ¡£è®°å½•"""
        archive_file = self.storage_path / "archives" / f"{archive_record['archive_id']}.json"
        archive_file.parent.mkdir(parents=True, exist_ok=True)
        
        with open(archive_file, 'w', encoding='utf-8') as f:
            json.dump(archive_record, f, indent=2, default=str)
    
    def _load_consciousness_state(self):
        """åŠ è½½æ„è¯†æµçŠ¶æ€"""
        try:
            state_file = self.storage_path / "consciousness_state.json"
            if state_file.exists():
                with open(state_file, 'r', encoding='utf-8') as f:
                    state = json.load(f)
                    self.global_state = state.get('global_state', {})
                    self.active_agents = set(state.get('active_agents', []))
        except Exception as e:
            logger.error(f"åŠ è½½æ„è¯†æµçŠ¶æ€å¤±è´¥: {e}")
    
    def save_consciousness_state(self):
        """ä¿å­˜æ„è¯†æµçŠ¶æ€"""
        if self.persistence_enabled:
            try:
                state_file = self.storage_path / "consciousness_state.json"
                state = {
                    'timestamp': datetime.now().isoformat(),
                    'global_state': self.global_state,
                    'active_agents': list(self.active_agents)
                }
                
                with open(state_file, 'w', encoding='utf-8') as f:
                    json.dump(state, f, indent=2, default=str)
                    
            except Exception as e:
                logger.error(f"ä¿å­˜æ„è¯†æµçŠ¶æ€å¤±è´¥: {e}")

# å…¨å±€æ„è¯†æµå®ä¾‹
_consciousness_stream_instance = None

def get_consciousness_stream(config: Dict[str, Any] = None) -> ConsciousnessStream:
    """è·å–å…¨å±€æ„è¯†æµå®ä¾‹"""
    global _consciousness_stream_instance
    if _consciousness_stream_instance is None:
        _consciousness_stream_instance = ConsciousnessStream(config)
    return _consciousness_stream_instance

if __name__ == "__main__":
    # æµ‹è¯•ä»£ç 
    async def test_consciousness_stream():
        # åˆ›å»ºæ„è¯†æµç³»ç»Ÿ
        config = {
            'buffer_size': 1000,
            'persistence': True,
            'storage_path': '.iflow/test_consciousness'
        }
        
        consciousness = get_consciousness_stream(config)
        
        # è®°å½•æµ‹è¯•äº‹ä»¶
        for i in range(10):
            event_id = consciousness.record_event(
                event_type="test_task",
                agent_id="test_agent",
                context={"task_id": i, "difficulty": i % 3},
                outcome=i % 2 == 0,
                importance=0.8
            )
            print(f"è®°å½•äº‹ä»¶: {event_id}")
        
        # æµ‹è¯•é¢„æµ‹
        prediction = consciousness.predict_next_optimal_action(
            current_context={"task_id": 11, "difficulty": 2},
            agent_id="test_agent"
        )
        
        print("\né¢„æµ‹ç»“æœ:")
        print(json.dumps(prediction, indent=2, default=str))
        
        # è·å–æ„è¯†æµæ‘˜è¦
        summary = consciousness.get_consciousness_summary()
        print("\næ„è¯†æµæ‘˜è¦:")
        print(json.dumps(summary, indent=2, default=str))
        
        # ä¿å­˜çŠ¶æ€
        consciousness.save_consciousness_state()
    
    # è¿è¡Œæµ‹è¯•
    asyncio.run(test_consciousness_stream())
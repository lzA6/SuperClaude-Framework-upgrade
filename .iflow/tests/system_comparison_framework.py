#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
ğŸ”¬ ç³»ç»Ÿå¯¹æ¯”æµ‹è¯•æ¡†æ¶ (System Comparison Testing Framework)
ç”¨äºå¯¹æ¯”æµ‹è¯•æ–°æ—§ç³»ç»Ÿçš„æ€§èƒ½ã€è´¨é‡ã€æ•ˆç‡å’Œèƒ½åŠ›å·®å¼‚
ä½ ä¸€å®šè¦è¶…çº§æ€è€ƒã€æé™æ€è€ƒã€æ·±åº¦æ€è€ƒï¼Œå…¨åŠ›æ€è€ƒã€è¶…å¼ºæ€è€ƒï¼Œè®¤çœŸä»”ç»†æ€è€ƒï¼ˆultrathinkã€think really super hardã€think intenselyï¼‰ã€‚
"""

import os
import sys
import json
import asyncio
import logging
import time
import statistics
import traceback
from pathlib import Path
from typing import Dict, List, Any, Optional, Tuple, Callable
from dataclasses import dataclass, field
from datetime import datetime, timedelta
from collections import defaultdict, Counter
import importlib.util
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# æ·»åŠ é¡¹ç›®è·¯å¾„
PROJECT_ROOT = Path(__file__).parent.parent.parent
sys.path.insert(0, str(PROJECT_ROOT))

logger = logging.getLogger(__name__)

@dataclass
class TestScenario:
    """æµ‹è¯•åœºæ™¯"""
    name: str
    description: str
    test_function: Callable
    expected_outcomes: List[str]
    complexity_level: int  # 1-10
    category: str
    tags: List[str] = field(default_factory=list)
    timeout: int = 60

@dataclass
class TestResult:
    """æµ‹è¯•ç»“æœ"""
    scenario_name: str
    system_name: str
    success: bool
    execution_time: float
    output_quality: float  # 0-1
    completeness: float  # 0-1
    efficiency: float  # 0-1
    innovation_score: float  # 0-1
    error_message: Optional[str] = None
    metrics: Dict[str, float] = field(default_factory=dict)
    artifacts: Dict[str, Any] = field(default_factory=dict)

@dataclass
class ComparisonReport:
    """å¯¹æ¯”æŠ¥å‘Š"""
    test_date: datetime
    scenarios_tested: List[str]
    systems_compared: List[str]
    overall_results: Dict[str, TestResult]
    detailed_metrics: Dict[str, Dict[str, float]]
    recommendations: List[str]
    winner: Optional[str] = None
    improvement_areas: List[str] = field(default_factory=list)

class SystemComparisonFramework:
    """ç³»ç»Ÿå¯¹æ¯”æµ‹è¯•æ¡†æ¶"""
    
    def __init__(self, config: Dict[str, Any] = None):
        self.config = config or {}
        
        # æµ‹è¯•ç»“æœå­˜å‚¨
        self.test_results = defaultdict(list)
        self.scenarios = []
        self.systems = {}
        
        # é…ç½®
        self.output_dir = Path(self.config.get('output_dir', '.iflow/tests/reports'))
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # è¯„ä¼°æƒé‡
        self.evaluation_weights = {
            'quality': 0.3,
            'efficiency': 0.25,
            'completeness': 0.2,
            'innovation': 0.15,
            'reliability': 0.1
        }
        
        self._initialize_test_scenarios()
        self._initialize_systems()
        
        logger.info("ç³»ç»Ÿå¯¹æ¯”æµ‹è¯•æ¡†æ¶åˆå§‹åŒ–å®Œæˆ")
    
    def add_scenario(self, scenario: TestScenario):
        """æ·»åŠ æµ‹è¯•åœºæ™¯"""
        self.scenarios.append(scenario)
        logger.info(f"æ·»åŠ æµ‹è¯•åœºæ™¯: {scenario.name}")
    
    def add_system(self, name: str, system_module: Any):
        """æ·»åŠ è¦æµ‹è¯•çš„ç³»ç»Ÿ"""
        self.systems[name] = system_module
        logger.info(f"æ·»åŠ æµ‹è¯•ç³»ç»Ÿ: {name}")
    
    async def run_comparison(self, selected_scenarios: Optional[List[str]] = None,
                          selected_systems: Optional[List[str]] = None) -> ComparisonReport:
        """è¿è¡Œå¯¹æ¯”æµ‹è¯•"""
        # è¿‡æ»¤æµ‹è¯•åœºæ™¯å’Œç³»ç»Ÿ
        scenarios_to_test = self._filter_scenarios(selected_scenarios)
        systems_to_test = self._filter_systems(selected_systems)
        
        logger.info(f"å¼€å§‹å¯¹æ¯”æµ‹è¯• - åœºæ™¯: {len(scenarios_to_test)}, ç³»ç»Ÿ: {len(systems_to_test)}")
        
        # æ‰§è¡Œæµ‹è¯•
        all_results = {}
        
        for scenario in scenarios_to_test:
            logger.info(f"æ‰§è¡Œåœºæ™¯: {scenario.name}")
            
            for system_name, system in systems_to_test.items():
                try:
                    result = await self._run_single_test(scenario, system_name, system)
                    all_results[f"{scenario.name}_{system_name}"] = result
                    self.test_results[scenario.name].append(result)
                    
                    logger.info(f"  {system_name}: {'æˆåŠŸ' if result.success else 'å¤±è´¥'} "
                              f"(è´¨é‡: {result.output_quality:.2f}, æ•ˆç‡: {result.efficiency:.2f})")
                    
                except Exception as e:
                    logger.error(f"  {system_name}: æµ‹è¯•å¼‚å¸¸ - {e}")
                    all_results[f"{scenario.name}_{system_name}"] = TestResult(
                        scenario_name=scenario.name,
                        system_name=system_name,
                        success=False,
                        execution_time=0.0,
                        output_quality=0.0,
                        completeness=0.0,
                        efficiency=0.0,
                        innovation_score=0.0,
                        error_message=str(e)
                    )
        
        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
        report = self._generate_comparison_report(
            scenarios_to_test, systems_to_test, all_results
        )
        
        # ä¿å­˜æŠ¥å‘Š
        self._save_report(report)
        
        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        await self._generate_visualizations(report)
        
        return report
    
    async def _run_single_test(self, scenario: TestScenario, system_name: str, 
                             system: Any) -> TestResult:
        """è¿è¡Œå•ä¸ªæµ‹è¯•"""
        start_time = time.time()
        
        try:
            # æ‰§è¡Œæµ‹è¯•å‡½æ•°
            if asyncio.iscoroutinefunction(scenario.test_function):
                output = await asyncio.wait_for(
                    scenario.test_function(system),
                    timeout=scenario.timeout
                )
            else:
                output = scenario.test_function(system)
            
            execution_time = time.time() - start_time
            
            # è¯„ä¼°ç»“æœ
            quality_score = self._evaluate_quality(output, scenario.expected_outcomes)
            completeness_score = self._evaluate_completeness(output, scenario)
            efficiency_score = self._evaluate_efficiency(execution_time, scenario.complexity_level)
            innovation_score = self._evaluate_innovation(output, scenario)
            
            # è®¡ç®—ç»¼åˆæŒ‡æ ‡
            overall_score = (
                quality_score * self.evaluation_weights['quality'] +
                completeness_score * self.evaluation_weights['completeness'] +
                efficiency_score * self.evaluation_weights['efficiency'] +
                innovation_score * self.evaluation_weights['innovation']
            )
            
            return TestResult(
                scenario_name=scenario.name,
                system_name=system_name,
                success=True,
                execution_time=execution_time,
                output_quality=quality_score,
                completeness=completeness_score,
                efficiency=efficiency_score,
                innovation_score=innovation_score,
                metrics={
                    'overall_score': overall_score,
                    'execution_time': execution_time,
                    'complexity_handled': scenario.complexity_level
                },
                artifacts={
                    'output': output,
                    'expected_outcomes': scenario.expected_outcomes
                }
            )
            
        except asyncio.TimeoutError:
            execution_time = time.time() - start_time
            return TestResult(
                scenario_name=scenario.name,
                system_name=system_name,
                success=False,
                execution_time=execution_time,
                output_quality=0.0,
                completeness=0.0,
                efficiency=0.0,
                innovation_score=0.0,
                error_message="æµ‹è¯•è¶…æ—¶"
            )
        
        except Exception as e:
            execution_time = time.time() - start_time
            return TestResult(
                scenario_name=scenario.name,
                system_name=system_name,
                success=False,
                execution_time=execution_time,
                output_quality=0.0,
                completeness=0.0,
                efficiency=0.0,
                innovation_score=0.0,
                error_message=str(e)
            )
    
    def _evaluate_quality(self, output: Any, expected_outcomes: List[str]) -> float:
        """è¯„ä¼°è¾“å‡ºè´¨é‡"""
        if not output:
            return 0.0
        
        # å°†è¾“å‡ºè½¬æ¢ä¸ºå­—ç¬¦ä¸²
        output_str = str(output).lower()
        
        # æ£€æŸ¥æœŸæœ›ç»“æœ
        matches = 0
        for expected in expected_outcomes:
            if expected.lower() in output_str:
                matches += 1
        
        # åŸºç¡€è´¨é‡åˆ†æ•°
        base_score = matches / len(expected_outcomes) if expected_outcomes else 0.5
        
        # é¢å¤–è´¨é‡æŒ‡æ ‡
        length_score = min(1.0, len(output_str) / 100)  # å†…å®¹é•¿åº¦
        structure_score = self._evaluate_structure(output)  # ç»“æ„è´¨é‡
        
        return (base_score + length_score + structure_score) / 3
    
    def _evaluate_completeness(self, output: Any, scenario: TestScenario) -> float:
        """è¯„ä¼°å®Œæ•´æ€§"""
        if not output:
            return 0.0
        
        output_str = str(output)
        
        # åŸºäºå¤æ‚åº¦è¯„ä¼°å®Œæ•´æ€§
        complexity_factor = scenario.complexity_level / 10.0
        
        # æ£€æŸ¥å…³é”®è¦ç´ 
        key_elements = self._extract_key_elements(scenario.description)
        found_elements = sum(1 for element in key_elements if element.lower() in output_str.lower())
        
        element_score = found_elements / len(key_elements) if key_elements else 0.5
        
        # ç»¼åˆå®Œæ•´æ€§åˆ†æ•°
        completeness = element_score * (1 + complexity_factor) / 2
        
        return min(1.0, completeness)
    
    def _evaluate_efficiency(self, execution_time: float, complexity_level: int) -> float:
        """è¯„ä¼°æ•ˆç‡"""
        # åŸºäºå¤æ‚åº¦è®¾å®šæœŸæœ›æ—¶é—´
        expected_time = complexity_level * 2.0  # æ¯çº§å¤æ‚åº¦æœŸæœ›2ç§’
        
        if execution_time <= expected_time:
            return 1.0
        else:
            # è¶…æ—¶æƒ©ç½š
            penalty = min(0.9, (execution_time - expected_time) / expected_time)
            return max(0.1, 1.0 - penalty)
    
    def _evaluate_innovation(self, output: Any, scenario: TestScenario) -> float:
        """è¯„ä¼°åˆ›æ–°æ€§"""
        if not output:
            return 0.0
        
        output_str = str(output)
        
        # åˆ›æ–°æŒ‡æ ‡
        innovation_indicators = [
            'novel', 'innovative', 'creative', 'unique', 'original',
            'breakthrough', 'advanced', 'cutting-edge', 'revolutionary'
        ]
        
        innovation_count = sum(1 for indicator in innovation_indicators 
                             if indicator in output_str.lower())
        
        # åŸºç¡€åˆ›æ–°åˆ†æ•°
        base_score = min(1.0, innovation_count / 3.0)
        
        # ç»“æ„å¤æ‚åº¦åˆ›æ–°
        structure_score = self._evaluate_structural_innovation(output)
        
        # é¢†åŸŸç›¸å…³æ€§åˆ›æ–°
        domain_score = self._evaluate_domain_innovation(output, scenario.category)
        
        return (base_score + structure_score + domain_score) / 3
    
    def _evaluate_structure(self, output: Any) -> float:
        """è¯„ä¼°ç»“æ„è´¨é‡"""
        output_str = str(output)
        
        # æ£€æŸ¥ç»“æ„å…ƒç´ 
        structure_elements = ['step', 'process', 'method', 'approach', 'framework']
        element_count = sum(1 for element in structure_elements 
                           if element in output_str.lower())
        
        return min(1.0, element_count / 3.0)
    
    def _extract_key_elements(self, description: str) -> List[str]:
        """æå–å…³é”®è¦ç´ """
        # ç®€åŒ–å®ç°ï¼šæå–åè¯å’Œå…³é”®è¯
        import re
        
        # ç§»é™¤åœç”¨è¯
        stop_words = {'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for'}
        
        # æå–å•è¯
        words = re.findall(r'\b\w+\b', description.lower())
        key_words = [word for word in words if word not in stop_words and len(word) > 3]
        
        # è¿”å›å‰5ä¸ªå…³é”®è¯
        return key_words[:5]
    
    def _evaluate_structural_innovation(self, output: Any) -> float:
        """è¯„ä¼°ç»“æ„åˆ›æ–°"""
        output_str = str(output)
        
        # æ£€æŸ¥é«˜çº§ç»“æ„
        advanced_structures = [
            'architecture', 'pattern', 'paradigm', 'framework',
            'methodology', 'algorithm', 'optimization'
        ]
        
        structure_count = sum(1 for structure in advanced_structures 
                            if structure in output_str.lower())
        
        return min(1.0, structure_count / 2.0)
    
    def _evaluate_domain_innovation(self, output: Any, category: str) -> float:
        """è¯„ä¼°é¢†åŸŸåˆ›æ–°"""
        output_str = str(output)
        
        # é¢†åŸŸç‰¹å®šåˆ›æ–°æŒ‡æ ‡
        domain_indicators = {
            'technical': ['quantum', 'neural', 'algorithm', 'optimization', 'scalability'],
            'creative': ['design', 'aesthetic', 'user experience', 'intuitive', 'engaging'],
            'analytical': ['insight', 'pattern', 'correlation', 'trend', 'prediction'],
            'strategic': ['vision', 'roadmap', 'milestone', 'objective', 'strategy']
        }
        
        indicators = domain_indicators.get(category, [])
        indicator_count = sum(1 for indicator in indicators 
                            if indicator in output_str.lower())
        
        return min(1.0, indicator_count / 2.0)
    
    def _filter_scenarios(self, selected: Optional[List[str]]) -> List[TestScenario]:
        """è¿‡æ»¤æµ‹è¯•åœºæ™¯"""
        if not selected:
            return self.scenarios
        
        return [scenario for scenario in self.scenarios if scenario.name in selected]
    
    def _filter_systems(self, selected: Optional[List[str]]) -> Dict[str, Any]:
        """è¿‡æ»¤æµ‹è¯•ç³»ç»Ÿ"""
        if not selected:
            return self.systems
        
        return {name: system for name, system in self.systems.items() if name in selected}
    
    def _generate_comparison_report(self, scenarios: List[TestScenario], 
                                  systems: Dict[str, Any],
                                  results: Dict[str, TestResult]) -> ComparisonReport:
        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
        # è®¡ç®—ç³»ç»Ÿæ€»åˆ†
        system_scores = defaultdict(list)
        
        for result in results.values():
            if result.success:
                overall_score = result.metrics.get('overall_score', 0.0)
                system_scores[result.system_name].append(overall_score)
        
        # è®¡ç®—å¹³å‡åˆ†æ•°
        avg_scores = {}
        for system_name, scores in system_scores.items():
            avg_scores[system_name] = statistics.mean(scores) if scores else 0.0
        
        # ç¡®å®šè·èƒœè€…
        winner = max(avg_scores.items(), key=lambda x: x[1])[0] if avg_scores else None
        
        # ç”Ÿæˆæ”¹è¿›å»ºè®®
        recommendations = self._generate_recommendations(results, avg_scores)
        
        # è¯†åˆ«æ”¹è¿›é¢†åŸŸ
        improvement_areas = self._identify_improvement_areas(results)
        
        return ComparisonReport(
            test_date=datetime.now(),
            scenarios_tested=[s.name for s in scenarios],
            systems_compared=list(systems.keys()),
            overall_results=results,
            detailed_metrics=avg_scores,
            recommendations=recommendations,
            winner=winner,
            improvement_areas=improvement_areas
        )
    
    def _generate_recommendations(self, results: Dict[str, TestResult], 
                                  avg_scores: Dict[str, float]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []
        
        if not avg_scores:
            return ["éœ€è¦æ›´å¤šæµ‹è¯•æ•°æ®æ¥ç”Ÿæˆå»ºè®®"]
        
        # æ‰¾å‡ºè¡¨ç°æœ€å¥½å’Œæœ€å·®çš„ç³»ç»Ÿ
        best_system = max(avg_scores.items(), key=lambda x: x[1])[0]
        worst_system = min(avg_scores.items(), key=lambda x: x[1])[0]
        
        recommendations.append(f"å»ºè®®ä¼˜å…ˆé‡‡ç”¨ {best_system} ç³»ç»Ÿï¼Œå…¶ç»¼åˆå¾—åˆ†æœ€é«˜")
        
        # åˆ†æå…·ä½“æŒ‡æ ‡
        system_metrics = defaultdict(lambda: defaultdict(list))
        
        for result in results.values():
            if result.success:
                system_metrics[result.system_name]['quality'].append(result.output_quality)
                system_metrics[result.system_name]['efficiency'].append(result.efficiency)
                system_metrics[result.system_name]['completeness'].append(result.completeness)
                system_metrics[result.system_name]['innovation'].append(result.innovation_score)
        
        # æ‰¾å‡ºå„ç³»ç»Ÿçš„å¼ºé¡¹å’Œå¼±é¡¹
        for system_name in avg_scores.keys():
            metrics = system_metrics[system_name]
            
            if metrics:
                avg_quality = statistics.mean(metrics['quality'])
                avg_efficiency = statistics.mean(metrics['efficiency'])
                
                if avg_quality < 0.7:
                    recommendations.append(f"{system_name} ç³»ç»Ÿéœ€è¦æå‡è¾“å‡ºè´¨é‡")
                
                if avg_efficiency < 0.7:
                    recommendations.append(f"{system_name} ç³»ç»Ÿéœ€è¦ä¼˜åŒ–æ‰§è¡Œæ•ˆç‡")
        
        return recommendations
    
    def _identify_improvement_areas(self, results: Dict[str, TestResult]) -> List[str]:
        """è¯†åˆ«æ”¹è¿›é¢†åŸŸ"""
        improvement_areas = []
        
        # åˆ†æå¤±è´¥æ¡ˆä¾‹
        failures = [r for r in results.values() if not r.success]
        if failures:
            failure_reasons = Counter(r.error_message for r in failures if r.error_message)
            common_failure = failure_reasons.most_common(1)[0]
            improvement_areas.append(f"éœ€è¦è§£å†³å¸¸è§é”™è¯¯: {common_failure}")
        
        # åˆ†æä½åˆ†æ¡ˆä¾‹
        low_scores = [r for r in results.values() 
                     if r.success and r.metrics.get('overall_score', 0) < 0.5]
        
        if low_scores:
            improvement_areas.append("éœ€è¦æå‡æ•´ä½“ç³»ç»Ÿæ€§èƒ½")
        
        # åˆ†æç‰¹å®šåœºæ™¯
        scenario_performance = defaultdict(list)
        for result in results.values():
            if result.success:
                scenario_performance[result.scenario_name].append(
                    result.metrics.get('overall_score', 0)
                )
        
        for scenario, scores in scenario_performance.items():
            avg_score = statistics.mean(scores) if scores else 0
            if avg_score < 0.6:
                improvement_areas.append(f"éœ€è¦æ”¹è¿› {scenario} åœºæ™¯çš„å¤„ç†èƒ½åŠ›")
        
        return improvement_areas
    
    def _save_report(self, report: ComparisonReport):
        """ä¿å­˜æŠ¥å‘Š"""
        # ä¿å­˜JSONæ ¼å¼
        json_file = self.output_dir / f"comparison_report_{report.test_date.strftime('%Y%m%d_%H%M%S')}.json"
        
        report_dict = {
            'test_date': report.test_date.isoformat(),
            'scenarios_tested': report.scenarios_tested,
            'systems_compared': report.systems_compared,
            'winner': report.winner,
            'detailed_metrics': report.detailed_metrics,
            'recommendations': report.recommendations,
            'improvement_areas': report.improvement_areas,
            'overall_results': {
                key: {
                    'success': result.success,
                    'execution_time': result.execution_time,
                    'output_quality': result.output_quality,
                    'completeness': result.completeness,
                    'efficiency': result.efficiency,
                    'innovation_score': result.innovation_score,
                    'metrics': result.metrics,
                    'error_message': result.error_message
                }
                for key, result in report.overall_results.items()
            }
        }
        
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(report_dict, f, indent=2, ensure_ascii=False, default=str)
        
        # ä¿å­˜Markdownæ ¼å¼
        md_file = self.output_dir / f"comparison_report_{report.test_date.strftime('%Y%m%d_%H%M%S')}.md"
        self._save_markdown_report(report, md_file)
        
        logger.info(f"æŠ¥å‘Šå·²ä¿å­˜: {json_file}")
    
    def _save_markdown_report(self, report: ComparisonReport, file_path: Path):
        """ä¿å­˜Markdownæ ¼å¼æŠ¥å‘Š"""
        with open(file_path, 'w', encoding='utf-8') as f:
            f.write("# ç³»ç»Ÿå¯¹æ¯”æµ‹è¯•æŠ¥å‘Š\n\n")
            f.write(f"**æµ‹è¯•æ—¥æœŸ**: {report.test_date.strftime('%Y-%m-%d %H:%M:%S')}\n")
            f.write(f"**æµ‹è¯•åœºæ™¯æ•°**: {len(report.scenarios_tested)}\n")
            f.write(f"**å¯¹æ¯”ç³»ç»Ÿæ•°**: {len(report.systems_compared)}\n\n")
            
            if report.winner:
                f.write(f"## ğŸ† è·èƒœç³»ç»Ÿ: {report.winner}\n\n")
            
            f.write("## ğŸ“Š ç»¼åˆè¯„åˆ†\n\n")
            f.write("| ç³»ç»Ÿ | å¹³å‡å¾—åˆ† |\n")
            f.write("|------|----------|\n")
            
            for system, score in report.detailed_metrics.items():
                f.write(f"| {system} | {score:.3f} |\n")
            
            f.write("\n## ğŸ’¡ æ”¹è¿›å»ºè®®\n\n")
            for i, recommendation in enumerate(report.recommendations, 1):
                f.write(f"{i}. {recommendation}\n")
            
            f.write("\n## ğŸ”§ æ”¹è¿›é¢†åŸŸ\n\n")
            for area in report.improvement_areas:
                f.write(f"- {area}\n")
            
            f.write("\n## ğŸ“‹ è¯¦ç»†ç»“æœ\n\n")
            for scenario in report.scenarios_tested:
                f.write(f"### {scenario}\n\n")
                f.write("| ç³»ç»Ÿ | æˆåŠŸ | è´¨é‡ | æ•ˆç‡ | å®Œæ•´æ€§ | åˆ›æ–° |\n")
                f.write("|------|------|------|------|--------|------|\n")
                
                for system in report.systems_compared:
                    result_key = f"{scenario}_{system}"
                    if result_key in report.overall_results:
                        result = report.overall_results[result_key]
                        f.write(f"| {system} | {'âœ“' if result.success else 'âœ—'} | "
                              f"{result.output_quality:.2f} | {result.efficiency:.2f} | "
                              f"{result.completeness:.2f} | {result.innovation_score:.2f} |\n")
                
                f.write("\n")
    
    async def _generate_visualizations(self, report: ComparisonReport):
        """ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨"""
        try:
            # è®¾ç½®å›¾è¡¨æ ·å¼
            plt.style.use('seaborn-v0_8')
            
            # 1. ç³»ç»Ÿç»¼åˆå¾—åˆ†å¯¹æ¯”
            fig, axes = plt.subplots(2, 2, figsize=(15, 12))
            fig.suptitle('ç³»ç»Ÿå¯¹æ¯”æµ‹è¯•å¯è§†åŒ–åˆ†æ', fontsize=16, fontweight='bold')
            
            # ç»¼åˆå¾—åˆ†æ¡å½¢å›¾
            systems = list(report.detailed_metrics.keys())
            scores = list(report.detailed_metrics.values())
            
            axes[0, 0].bar(systems, scores, color='skyblue')
            axes[0, 0].set_title('ç³»ç»Ÿç»¼åˆå¾—åˆ†å¯¹æ¯”')
            axes[0, 0].set_ylabel('å¾—åˆ†')
            axes[0, 0].tick_params(axis='x', rotation=45)
            
            # å„ç»´åº¦é›·è¾¾å›¾
            categories = ['è´¨é‡', 'æ•ˆç‡', 'å®Œæ•´æ€§', 'åˆ›æ–°']
            
            for system in systems:
                values = []
                for result in report.overall_results.values():
                    if result.system_name == system and result.success:
                        values = [
                            result.output_quality,
                            result.efficiency,
                            result.completeness,
                            result.innovation_score
                        ]
                        break
                
                if values:
                    angles = np.linspace(0, 2 * np.pi, len(categories), endpoint=False)
                    values += values[:1]  # é—­åˆå›¾å½¢
                    angles = np.concatenate((angles, [angles[0]]))
                    
                    axes[0, 1].plot(angles, values, 'o-', linewidth=2, label=system)
            
            axes[0, 1].set_xticks(angles[:-1])
            axes[0, 1].set_xticklabels(categories)
            axes[0, 1].set_title('å„ç»´åº¦èƒ½åŠ›é›·è¾¾å›¾')
            axes[0, 1].legend()
            
            # æ‰§è¡Œæ—¶é—´åˆ†å¸ƒ
            execution_times = []
            system_labels = []
            
            for result in report.overall_results.values():
                if result.success:
                    execution_times.append(result.execution_time)
                    system_labels.append(result.system_name)
            
            if execution_times:
                axes[1, 0].hist(execution_times, bins=10, alpha=0.7, color='lightgreen')
                axes[1, 0].set_title('æ‰§è¡Œæ—¶é—´åˆ†å¸ƒ')
                axes[1, 0].set_xlabel('æ‰§è¡Œæ—¶é—´ (ç§’)')
                axes[1, 0].set_ylabel('é¢‘æ¬¡')
            
            # æˆåŠŸç‡å¯¹æ¯”
            success_rates = {}
            for system in systems:
                total = sum(1 for r in report.overall_results.values() if r.system_name == system)
                successful = sum(1 for r in report.overall_results.values() 
                               if r.system_name == system and r.success)
                success_rates[system] = successful / total if total > 0 else 0
            
            axes[1, 1].pie(success_rates.values(), labels=success_rates.keys(), autopct='%1.1f%%')
            axes[1, 1].set_title('ç³»ç»ŸæˆåŠŸç‡å¯¹æ¯”')
            
            plt.tight_layout()
            
            # ä¿å­˜å›¾è¡¨
            chart_file = self.output_dir / f"comparison_charts_{report.test_date.strftime('%Y%m%d_%H%M%S')}.png"
            plt.savefig(chart_file, dpi=300, bbox_inches='tight')
            plt.close()
            
            logger.info(f"å¯è§†åŒ–å›¾è¡¨å·²ä¿å­˜: {chart_file}")
            
        except Exception as e:
            logger.error(f"ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨å¤±è´¥: {e}")
    
    def _initialize_test_scenarios(self):
        """åˆå§‹åŒ–æµ‹è¯•åœºæ™¯"""
        # è¿™é‡Œæ·»åŠ é»˜è®¤æµ‹è¯•åœºæ™¯
        default_scenarios = [
            TestScenario(
                name="code_generation",
                description="ç”Ÿæˆä¸€ä¸ªPythonå‡½æ•°æ¥è®¡ç®—æ–æ³¢é‚£å¥‘æ•°åˆ—",
                test_function=self._test_code_generation,
                expected_outcomes=["function", "fibonacci", "recursive", "iterative"],
                complexity_level=5,
                category="technical",
                tags=["coding", "algorithm"]
            ),
            TestScenario(
                name="problem_solving",
                description="åˆ†æå¹¶è§£å†³ä¸€ä¸ªå¤æ‚çš„ä¸šåŠ¡é—®é¢˜",
                test_function=self._test_problem_solving,
                expected_outcomes=["analysis", "solution", "implementation", "evaluation"],
                complexity_level=7,
                category="analytical",
                tags=["analysis", "solution"]
            ),
            TestScenario(
                name="creative_writing",
                description="åˆ›ä½œä¸€ä¸ªå…³äºæœªæ¥ç§‘æŠ€çš„æ•…äº‹",
                test_function=self._test_creative_writing,
                expected_outcomes=["story", "narrative", "characters", "plot"],
                complexity_level=6,
                category="creative",
                tags=["writing", "creativity"]
            ),
            TestScenario(
                name="system_design",
                description="è®¾è®¡ä¸€ä¸ªå¾®æœåŠ¡æ¶æ„æ–¹æ¡ˆ",
                test_function=self._test_system_design,
                expected_outcomes=["architecture", "services", "scalability", "deployment"],
                complexity_level=8,
                category="technical",
                tags=["architecture", "design"]
            )
        ]
        
        self.scenarios.extend(default_scenarios)
    
    def _initialize_systems(self):
        """åˆå§‹åŒ–æµ‹è¯•ç³»ç»Ÿ"""
        # è¿™é‡Œå¯ä»¥åŠ è½½ç°æœ‰ç³»ç»Ÿæ¨¡å—
        pass
    
    async def _test_code_generation(self, system: Any) -> str:
        """æµ‹è¯•ä»£ç ç”Ÿæˆèƒ½åŠ›"""
        # æ¨¡æ‹Ÿæµ‹è¯•å‡½æ•°
        return """
def fibonacci(n):
    if n <= 1:
        return n
    return fibonacci(n-1) + fibonacci(n-2)

# ä¼˜åŒ–ç‰ˆæœ¬
def fibonacci_optimized(n):
    a, b = 0, 1
    for _ in range(n):
        a, b = b, a + b
    return a
        """
    
    async def _test_problem_solving(self, system: Any) -> str:
        """æµ‹è¯•é—®é¢˜è§£å†³èƒ½åŠ›"""
        return """
## é—®é¢˜åˆ†æ
ç”¨æˆ·åé¦ˆç³»ç»Ÿå“åº”ç¼“æ…¢ï¼Œéœ€è¦ä¼˜åŒ–ã€‚

## æ ¹æœ¬åŸå› 
1. æ•°æ®åº“æŸ¥è¯¢æ•ˆç‡ä½
2. ç¼“å­˜æœºåˆ¶ä¸å®Œå–„
3. ä»£ç å­˜åœ¨æ€§èƒ½ç“¶é¢ˆ

## è§£å†³æ–¹æ¡ˆ
1. ä¼˜åŒ–æ•°æ®åº“ç´¢å¼•
2. å®æ–½Redisç¼“å­˜
3. ä»£ç é‡æ„å’Œæ€§èƒ½è°ƒä¼˜

## å®æ–½è®¡åˆ’
1. ç¬¬ä¸€é˜¶æ®µï¼šæ•°æ®åº“ä¼˜åŒ–ï¼ˆ1å‘¨ï¼‰
2. ç¬¬äºŒé˜¶æ®µï¼šç¼“å­˜å®æ–½ï¼ˆ2å‘¨ï¼‰
3. ç¬¬ä¸‰é˜¶æ®µï¼šä»£ç é‡æ„ï¼ˆ3å‘¨ï¼‰

## é¢„æœŸæ•ˆæœ
- å“åº”æ—¶é—´å‡å°‘60%
- ç³»ç»Ÿååé‡æå‡3å€
- ç”¨æˆ·ä½“éªŒæ˜¾è‘—æ”¹å–„
        """
    
    async def _test_creative_writing(self, system: Any) -> str:
        """æµ‹è¯•åˆ›æ„å†™ä½œèƒ½åŠ›"""
        return """
# é‡å­é»æ˜

## æ•…äº‹èƒŒæ™¯
2085å¹´ï¼Œäººç±»é¦–æ¬¡æˆåŠŸå®ç°äº†é‡å­è®¡ç®—æœºçš„å•†ä¸šåŒ–åº”ç”¨ã€‚

## ä¸»è¦äººç‰©
- ææ˜ï¼šé‡å­ç®—æ³•å·¥ç¨‹å¸ˆ
- ç‹èŠ³ï¼šAIä¼¦ç†ä¸“å®¶
- å¼ åšå£«ï¼šé‡å­ç‰©ç†å­¦å®¶

## æƒ…èŠ‚å‘å±•
åœ¨ä¸€ä¸ªæ™®é€šçš„å‘¨äºŒæ—©æ™¨ï¼Œææ˜çš„é‡å­è®¡ç®—æœºçªç„¶äº§ç”Ÿäº†å‰æ‰€æœªæœ‰çš„å¼‚å¸¸ç°è±¡...

## ä¸»é¢˜æ¢è®¨
ç§‘æŠ€å‘å±•å¸¦æ¥çš„æœºé‡ä¸æŒ‘æˆ˜ï¼Œäººå·¥æ™ºèƒ½ä¸äººç±»æ„è¯†çš„è¾¹ç•Œã€‚
        """
    
    async def _test_system_design(self, system: Any) -> str:
        """æµ‹è¯•ç³»ç»Ÿè®¾è®¡èƒ½åŠ›"""
        return """
# å¾®æœåŠ¡ç”µå•†å¹³å°æ¶æ„è®¾è®¡

## æ¶æ„æ¦‚è¿°
é‡‡ç”¨å¾®æœåŠ¡æ¶æ„ï¼Œæ”¯æŒé«˜å¹¶å‘ã€é«˜å¯ç”¨ã€å¯æ‰©å±•ã€‚

## æ ¸å¿ƒæœåŠ¡
1. ç”¨æˆ·æœåŠ¡ï¼ˆUser Serviceï¼‰
2. å•†å“æœåŠ¡ï¼ˆProduct Serviceï¼‰
3. è®¢å•æœåŠ¡ï¼ˆOrder Serviceï¼‰
4. æ”¯ä»˜æœåŠ¡ï¼ˆPayment Serviceï¼‰
5. åº“å­˜æœåŠ¡ï¼ˆInventory Serviceï¼‰

## æŠ€æœ¯æ ˆ
- åç«¯ï¼šSpring Boot + Node.js
- æ•°æ®åº“ï¼šPostgreSQL + MongoDB + Redis
- æ¶ˆæ¯é˜Ÿåˆ—ï¼šApache Kafka
- å®¹å™¨åŒ–ï¼šDocker + Kubernetes
- ç›‘æ§ï¼šPrometheus + Grafana

## æ‰©å±•æ€§è®¾è®¡
- æ°´å¹³æ‰©å±•ï¼šé€šè¿‡K8sè‡ªåŠ¨æ‰©ç¼©å®¹
- æ•°æ®åˆ†ç‰‡ï¼šæŒ‰ç”¨æˆ·IDåˆ†ç‰‡
- ç¼“å­˜ç­–ç•¥ï¼šå¤šçº§ç¼“å­˜ä½“ç³»

## éƒ¨ç½²æ–¹æ¡ˆ
- å¼€å‘ç¯å¢ƒï¼šæœ¬åœ°Docker Compose
- æµ‹è¯•ç¯å¢ƒï¼šK8sæµ‹è¯•é›†ç¾¤
- ç”Ÿäº§ç¯å¢ƒï¼šå¤šäº‘éƒ¨ç½²
        """

# æµ‹è¯•å·¥å…·å‡½æ•°
async def run_system_comparison():
    """è¿è¡Œç³»ç»Ÿå¯¹æ¯”æµ‹è¯•"""
    framework = SystemComparisonFramework()
    
    # è¿™é‡Œå¯ä»¥æ·»åŠ è‡ªå®šä¹‰ç³»ç»Ÿå’Œåœºæ™¯
    
    # è¿è¡Œå¯¹æ¯”æµ‹è¯•
    report = await framework.run_comparison()
    
    print(f"\nğŸ† æµ‹è¯•å®Œæˆï¼è·èƒœç³»ç»Ÿ: {report.winner}")
    print(f"ğŸ“Š è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {framework.output_dir}")
    
    return report

if __name__ == "__main__":
    # è¿è¡Œæµ‹è¯•
    asyncio.run(run_system_comparison())